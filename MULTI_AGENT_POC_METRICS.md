# Система метрик и методология сравнительного анализа для POC Multi-Agent системы

## 1. Обзор системы метрик

### Назначение и цели
Система метрик предназначена для объективного сравнения эффективности Multi-Agent и Single-Agent подходов в решении задач разработки Flutter приложений. Основные цели:
- Измерение качества решений (Task Success Rate)
- Оценка скорости решения задач (Time To Useful Answer)
- Анализ надежности и точности (Hallucination Rate)
- Расчет экономической эффективности (Cost per Successful Task)
- Обеспечение статистической значимости результатов

### Принципы сбора метрик
- **Автоматизация**: Максимальная автоматизация сбора данных для минимизации человеческого фактора
- **Воспроизводимость**: Фиксированные условия тестирования (модель LLM, температура, параметры)
- **Комплексность**: Многоуровневый анализ (количественный + качественный)
- **Статистическая значимость**: Минимум 40 задач для обеспечения статистической мощности

### Архитектура системы метрик
Система состоит из следующих компонентов:
- **MetricsCollector**: Сбор сырых данных из логов и API
- **TokenCounter**: Подсчет токенов для cost анализа
- **TimeTracker**: Измерение временных метрик
- **QualityEvaluator**: Автоматическая оценка качества решений
- **MetricsStorage**: Хранение данных в PostgreSQL или SQLite

## 2. Детальное описание метрик

### 2.1 Task Success Rate (TSR)
- **Определение**: процент успешно выполненных задач
- **Формула расчета**: `TSR = (успешные задачи / всего задач) × 100%`
- **Критерии успеха задачи**:
  - Все файлы созданы/изменены корректно
  - Код компилируется без ошибок
  - Тесты проходят (если применимо)
  - Нет hallucinated imports/API
  - Задача решена полностью
- **Способ измерения**: автоматические проверки + ручная валидация
- **Целевое значение**: Multi-agent TSR ≥ Single-agent TSR + 15%

### 2.2 Time To Useful Answer (TTUA)
- **Определение**: время от начала задачи до получения полезного результата
- **Компоненты времени**:
  - Time to first token
  - Time to first tool call
  - Time to completion
  - Total wall-clock time
- **Способ измерения**: timestamps в логах
- **Целевое значение**: Multi-agent TTUA ≤ Single-agent TTUA × 1.2 (допустим 20% overhead)

### 2.3 Hallucination Rate
- **Определение**: частота галлюцинаций (несуществующие API, файлы, функции)
- **Типы галлюцинаций**:
  - Несуществующие imports
  - Несуществующие API методы
  - Несуществующие файлы
  - Неверные параметры функций
- **Способ измерения**: автоматический анализ кода + статический анализ
- **Целевое значение**: Multi-agent Hallucination Rate ≤ Single-agent × 0.5 (снижение в 2 раза)

### 2.4 Cost per Successful Task
- **Определение**: стоимость выполнения одной успешной задачи
- **Компоненты стоимости**:
  - Input tokens × цена
  - Output tokens × цена
  - Количество LLM вызовов
- **Формула**: `Cost = (Σ input_tokens × $0.003 + Σ output_tokens × $0.015) / 1000`
- **Способ измерения**: подсчет токенов из LLM API
- **Целевое значение**: Multi-agent Cost ≤ Single-agent Cost × 1.3 (допустимо 30% увеличение при значительном улучшении качества)

## 3. Дополнительные метрики

### 3.1 Iteration Count
- Количество итераций LLM для решения задачи
- Среднее, медиана, 95-й перцентиль

### 3.2 Tool Usage Metrics
- Количество вызовов каждого инструмента
- Успешность вызовов инструментов
- Время выполнения инструментов

### 3.3 Multi-Agent специфичные метрики
- Количество переключений агентов
- Точность классификации Orchestrator
- Overhead на маршрутизацию
- Распределение задач по агентам

### 3.4 Quality Metrics
- Code quality score (если применимо)
- Completeness score (полнота решения)
- Correctness score (корректность решения)

## 4. Архитектура сбора метрик

### 4.1 Компоненты системы
```
MetricsCollector
├── TaskTracker - отслеживание жизненного цикла задачи
├── TokenCounter - подсчет токенов
├── TimeTracker - измерение времени
├── QualityEvaluator - оценка качества
└── MetricsStorage - хранение данных
```

### 4.2 Схема базы данных
```sql
-- Таблица экспериментов
CREATE TABLE experiments (
    id TEXT PRIMARY KEY, -- UUID as TEXT for SQLite
    mode TEXT, -- 'single-agent' или 'multi-agent'
    started_at TEXT, -- TIMESTAMP as TEXT for SQLite
    completed_at TEXT,
    config TEXT -- JSON as TEXT for SQLite
);

-- Таблица выполнения задач
CREATE TABLE task_executions (
    id TEXT PRIMARY KEY,
    experiment_id TEXT REFERENCES experiments(id),
    task_id TEXT, -- из benchmark
    mode TEXT,
    started_at TEXT,
    completed_at TEXT,
    success INTEGER, -- BOOLEAN as INTEGER in SQLite
    failure_reason TEXT,
    metrics TEXT -- JSON as TEXT
);

-- Таблица LLM вызовов
CREATE TABLE llm_calls (
    id TEXT PRIMARY KEY,
    task_execution_id TEXT REFERENCES task_executions(id),
    agent_type TEXT,
    started_at TEXT,
    completed_at TEXT,
    input_tokens INTEGER,
    output_tokens INTEGER,
    model TEXT
);

-- Таблица вызовов инструментов
CREATE TABLE tool_calls (
    id TEXT PRIMARY KEY,
    task_execution_id TEXT REFERENCES task_executions(id),
    tool_name TEXT,
    started_at TEXT,
    completed_at TEXT,
    success INTEGER,
    error TEXT
);

-- Таблица переключений агентов
CREATE TABLE agent_switches (
    id TEXT PRIMARY KEY,
    task_execution_id TEXT REFERENCES task_executions(id),
    from_agent TEXT,
    to_agent TEXT,
    reason TEXT,
    timestamp TEXT
);
```

### 4.3 Точки сбора метрик
- При старте задачи
- При каждом LLM вызове
- При каждом вызове инструмента
- При переключении агента (multi-agent)
- При завершении задачи

## 5. Методология сравнительного анализа

### 5.1 Дизайн эксперимента
- **A/B тестирование**: каждая задача выполняется в обоих режимах
- **Рандомизация**: порядок выполнения задач рандомизирован
- **Повторения**: минимум 3 прогона для статистической значимости
- **Контроль переменных**: одинаковые LLM модели, температура, параметры

### 5.2 Статистический анализ
- **Описательная статистика**: среднее, медиана, стандартное отклонение
- **Тесты значимости**: t-test для сравнения средних
- **Доверительные интервалы**: 95% CI для всех метрик
- **Effect size**: Cohen's d для оценки практической значимости

### 5.3 Сегментация анализа
Анализировать метрики отдельно для:
- Категорий задач (простые/средние/сложные)
- Типов задач (coding/architecture/debug/question)
- Временных периодов (начало/середина/конец эксперимента)

### 5.4 Визуализация результатов
- **Сравнительные таблицы**: side-by-side метрики
- **Box plots**: распределение метрик
- **Scatter plots**: корреляции между метриками
- **Time series**: динамика метрик во времени
- **Heatmaps**: успешность по категориям задач

## 6. Формат отчета о результатах

### 6.1 Executive Summary
- Ключевые выводы (1 страница)
- Рекомендация: продолжать или остановить

### 6.2 Детальные результаты
- Таблица сравнения по всем метрикам
- Статистическая значимость
- Графики и визуализации

### 6.3 Анализ по категориям
- Где multi-agent лучше
- Где single-agent лучше
- Объяснение различий

### 6.4 Качественный анализ
- Типичные паттерны поведения
- Интересные кейсы
- Проблемы и ограничения

### 6.5 Рекомендации
- Следующие шаги
- Области для улучшения
- Roadmap развития

## 7. Инструменты и технологии

### 7.1 Сбор данных
- Python logging для событий
- PostgreSQL или SQLite для хранения метрик
- Prometheus для real-time метрик (опционально)

### 7.2 Анализ данных
- Pandas для обработки данных
- SciPy для статистического анализа
- Matplotlib/Seaborn для визуализации

### 7.3 Отчетность
- Jupyter Notebook для интерактивного анализа
- Markdown для финального отчета
- Grafana для dashboard (опционально)

## 8. Чеклист валидации метрик

Перед началом эксперимента проверить:
- [ ] Все метрики собираются корректно
- [ ] База данных настроена и доступна
- [ ] Timestamps синхронизированы
- [ ] Автоматические проверки работают
- [ ] Есть backup данных
- [ ] Документация актуальна

---

**Создано:** Architect Mode  
**Дата:** 2026-01-12  
**Статус:** Готов к использованию