services:
  gateway:
    build:
      context: ./gateway
      dockerfile: Dockerfile
    ports:
      - "${GATEWAY_PORT}:${GATEWAY_PORT}"
    environment:
      - ENVIRONMENT=${ENVIRONMENT}
      - PORT=${GATEWAY_PORT}
      - AGENT_URL=http://agent-runtime:${AGENT_RUNTIME_PORT}
      - LOG_LEVEL=${LOG_LEVEL}
      - INTERNAL_API_KEY=${INTERNAL_API_KEY}
      - WS_HEARTBEAT_INTERVAL=${WS_HEARTBEAT_INTERVAL}
      - WS_CLOSE_TIMEOUT=${WS_CLOSE_TIMEOUT}
      - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT}
    depends_on:
      agent-runtime:
        condition: service_healthy
    networks:
      - codelab-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${GATEWAY_PORT}/health"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: ${HEALTHCHECK_START_PERIOD}

  agent-runtime:
    build:
      context: ./agent-runtime
      dockerfile: Dockerfile
    ports:
      - "${AGENT_RUNTIME_PORT}:${AGENT_RUNTIME_PORT}"
    environment:
      - ENVIRONMENT=${ENVIRONMENT}
      - PORT=${AGENT_RUNTIME_PORT}
      - LLM_PROXY_URL=http://llm-proxy:${LLM_PROXY_PORT}
      - LOG_LEVEL=${LOG_LEVEL}
      - INTERNAL_API_KEY=${INTERNAL_API_KEY}
      - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT}
      - LLM_MODEL=${LLM_MODEL}
    depends_on:
      llm-proxy:
        condition: service_healthy
    networks:
      - codelab-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${AGENT_RUNTIME_PORT}/health"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: ${HEALTHCHECK_START_PERIOD}

  llm-proxy:
    build:
      context: ./llm-proxy
      dockerfile: Dockerfile
    ports:
      - "${LLM_PROXY_PORT}:${LLM_PROXY_PORT}"
    environment:
      - ENVIRONMENT=${ENVIRONMENT}
      - PORT=${LLM_PROXY_PORT}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - LLM_MODE=${LLM_MODE}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL}
      - LOG_LEVEL=${LOG_LEVEL}
      - INTERNAL_API_KEY=${INTERNAL_API_KEY}
      - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT}
      - VLLM_API_KEY=${VLLM_API_KEY}
      - VLLM_BASE_URL=http://vllm:8003/v1
    networks:
      - codelab-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${LLM_PROXY_PORT}/health"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: ${HEALTHCHECK_START_PERIOD}

  vllm:
    image: vllm-cpu:latest
    privileged: true
    shm_size: 4g
    # Apple Sillicon M4
    environment:
      - LOG_LEVEL=DEBUG
      - VLLM_CPU_KVCACHE_SPACE=4
      - VLLM_CPU_OMP_THREADS_BIND=2
    #- HF_TOKEN=${HF_TOKEN}  # Можно экспортировать на хосте или прописать прямо здесь свой токен
    command:
      [
        "--model", "Qwen/Qwen3-0.6B",
        "--host", "0.0.0.0",
        "--port", "8003",
        "--max-model-len", "32768"
      ]

    # Apple Sillicon M1
    #environment:
    #  - LOG_LEVEL=DEBUG
    #  - VLLM_CPU_KVCACHE_SPACE=10
    #  - VLLM_CPU_OMP_THREADS_BIND=2
    ##- HF_TOKEN=${HF_TOKEN}  # Можно экспортировать на хосте или прописать прямо здесь свой токен
    #command:
    #  [
    #    "--model", "Qwen/Qwen3-0.6B",
    #    "--dtype", "float32",
    #    "--enforce-eager",
    #    "--host", "0.0.0.0",
    #    "--port", "8003",
    #    "--max-model-len", "32768",
    #    "--max-num-seqs", "16"
    #  ]


    volumes:
      - ./vllm-models/:/models/
      - ./vllm-hf-cache:/root/.cache/huggingface
    ports:
      - "8003:8003"
    networks:
      - codelab-network

networks:
  codelab-network:
    driver: bridge
