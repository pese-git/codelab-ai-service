services:
  gateway:
    build:
      context: ./gateway
      dockerfile: Dockerfile
    ports:
      - "${GATEWAY_PORT}:${GATEWAY_PORT}"
    environment:
      - ENVIRONMENT=${ENVIRONMENT}
      - PORT=${GATEWAY_PORT}
      - GATEWAY__AGENT_URL=http://agent-runtime:${AGENT_RUNTIME_PORT}
      - GATEWAY__LOG_LEVEL=${GATEWAY__LOG_LEVEL}
      - GATEWAY__INTERNAL_API_KEY=${GATEWAY__INTERNAL_API_KEY}
      - GATEWAY__WS_HEARTBEAT_INTERVAL=${GATEWAY__WS_HEARTBEAT_INTERVAL}
      - GATEWAY__WS_CLOSE_TIMEOUT=${GATEWAY__WS_CLOSE_TIMEOUT}
      - GATEWAY__MAX_CONCURRENT_REQUESTS=${GATEWAY__MAX_CONCURRENT_REQUESTS}
      - GATEWAY__REQUEST_TIMEOUT=${GATEWAY__REQUEST_TIMEOUT}
    depends_on:
      agent-runtime:
        condition: service_healthy
    networks:
      - codelab-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${GATEWAY_PORT}/health"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: ${HEALTHCHECK_START_PERIOD}

  agent-runtime:
    build:
      context: ./agent-runtime
      dockerfile: Dockerfile
    ports:
      - "${AGENT_RUNTIME_PORT}:${AGENT_RUNTIME_PORT}"
    volumes:
      # Монтируем директорию data для персистентности SQLite базы данных
      - ./agent-runtime/data:/app/data
    environment:
      - ENVIRONMENT=${ENVIRONMENT}
      - PORT=${AGENT_RUNTIME_PORT}
      - AGENT_RUNTIME__GATEWAY_URL=${AGENT_RUNTIME__GATEWAY_URL}
      - AGENT_RUNTIME__LLM_PROXY_URL=http://llm-proxy:${LLM_PROXY_PORT}
      - AGENT_RUNTIME__LOG_LEVEL=${AGENT_RUNTIME__LOG_LEVEL}
      - AGENT_RUNTIME__INTERNAL_API_KEY=${AGENT_RUNTIME__INTERNAL_API_KEY}
      - AGENT_RUNTIME__MAX_CONCURRENT_REQUESTS=${AGENT_RUNTIME__MAX_CONCURRENT_REQUESTS}
      - AGENT_RUNTIME__REQUEST_TIMEOUT=${AGENT_RUNTIME__REQUEST_TIMEOUT}
      - AGENT_RUNTIME__LLM_MODEL=${AGENT_RUNTIME__LLM_MODEL}
      # Путь к базе данных SQLite (будет храниться на хост-машине)
      - AGENT_RUNTIME__DB_URL=sqlite:///data/agent_runtime.db
    depends_on:
      llm-proxy:
        condition: service_healthy
    networks:
      - codelab-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${AGENT_RUNTIME_PORT}/health"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: ${HEALTHCHECK_START_PERIOD}

  llm-proxy:
    build:
      context: ./llm-proxy
      dockerfile: Dockerfile
    ports:
      - "${LLM_PROXY_PORT}:${LLM_PROXY_PORT}"
    environment:
      - ENVIRONMENT=${ENVIRONMENT}
      - PORT=${LLM_PROXY_PORT}
      - LLM_PROXY__LITELLM_PROXY_URL=http://litellm-proxy:4000
      - LLM_PROXY__LITELLM_API_KEY=${LITELLM_MASTER_KEY}
      - LLM_PROXY__DEFAULT_MODEL=${LLM_PROXY__DEFAULT_MODEL}
      - LLM_PROXY__LLM_MODE=${LLM_PROXY__LLM_MODE}
      - LLM_PROXY__LOG_LEVEL=${LLM_PROXY__LOG_LEVEL}
      - LLM_PROXY__INTERNAL_API_KEY=${LLM_PROXY__INTERNAL_API_KEY}
      - LLM_PROXY__MAX_CONCURRENT_REQUESTS=${LLM_PROXY__MAX_CONCURRENT_REQUESTS}
      - LLM_PROXY__REQUEST_TIMEOUT=${LLM_PROXY__REQUEST_TIMEOUT}
    depends_on:
      litellm-proxy:
        condition: service_healthy
    networks:
      - codelab-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${LLM_PROXY_PORT}/health"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: ${HEALTHCHECK_START_PERIOD}

  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - "4000:4000"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - AZURE_API_KEY=${AZURE_API_KEY}
      - AZURE_API_BASE=${AZURE_API_BASE}
      - AZURE_API_VERSION=${AZURE_API_VERSION}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - OLLAMA_API_BASE=http://ollama:11434
    command: ["--config", "/app/config.yaml", "--port", "4000", "--detailed_debug"]
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - codelab-network
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4000/').read()"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: ${HEALTHCHECK_START_PERIOD}

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    networks:
      - codelab-network
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: ${HEALTHCHECK_INTERVAL}
      timeout: ${HEALTHCHECK_TIMEOUT}
      retries: ${HEALTHCHECK_RETRIES}
      start_period: 30s

#  vllm:
#    image: vllm-cpu:latest
#    privileged: true
#    shm_size: 4g
#    # Apple Sillicon M4
#    environment:
#      - LOG_LEVEL=DEBUG
#      - VLLM_CPU_KVCACHE_SPACE=4
#      - VLLM_CPU_OMP_THREADS_BIND=2
#    #- HF_TOKEN=${HF_TOKEN}  # Можно экспортировать на хосте или прописать прямо здесь свой токен
#    command:
#      [
#        "--model", "Qwen/Qwen3-0.6B",
#        "--host", "0.0.0.0",
#        "--port", "8003",
#        "--max-model-len", "32768"
#      ]
#
#    # Apple Sillicon M1
#    #environment:
#    #  - LOG_LEVEL=DEBUG
#    #  - VLLM_CPU_KVCACHE_SPACE=10
#    #  - VLLM_CPU_OMP_THREADS_BIND=2
#    ##- HF_TOKEN=${HF_TOKEN}  # Можно экспортировать на хосте или прописать прямо здесь свой токен
#    #command:
#    #  [
#    #    "--model", "Qwen/Qwen3-0.6B",
#    #    "--dtype", "float32",
#    #    "--enforce-eager",
#    #    "--host", "0.0.0.0",
#    #    "--port", "8003",
#    #    "--max-model-len", "32768",
#    #    "--max-num-seqs", "16"
#    #  ]
#
#
#    volumes:
#      - ./vllm-models/:/models/
#      - ./vllm-hf-cache:/root/.cache/huggingface
#    ports:
#      - "8003:8003"
#    networks:
#      - codelab-network

volumes:
  ollama-data:
    driver: local

networks:
  codelab-network:
    driver: bridge
